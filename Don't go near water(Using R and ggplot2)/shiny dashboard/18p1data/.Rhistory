names( machine_header_info )
csv_path = 'D:/prophecy/rpi code/1516279800.csv'
subassembly_instance_list_info = lapply( machine_info$models$assembly$subAssemblyInstances, function( current_subassembly_instance ){
current_subassembly_instance_name = current_subassembly_instance$name ; current_subassembly_name = current_subassembly_instance$subAssembly$name
current_collectors_list = current_subassembly_instance$subAssembly$collectors
current_collectors_name = sapply( current_collectors_list, function( current_collector ){
current_collector_name = current_collector[["name"]]
return(  )
}, USE.NAMES = F )
return( list( 'subassembly_instance' = current_subassembly_instance_name, 'subassembly' = current_subassembly_name, 'collectors' = current_collectors_name ) )
} )
csv_path = 'D:/prophecy/rpi code/1516279800.csv'
All_Type_df = function( csv_path ) {
df = read.csv( csv_path, header = F, stringsAsFactors = F )
collector_types = sapply( as.character( df$V2 ), function( element ){
collector_name = unlist( strsplit( element, "/" ) )[2]
}, USE.NAMES = F )
unique_collectors = unique( collector_types )
df_for_type_all = lapply( unique_collectors, function( current_collector_type ){
collector_df = df[ which( collector_types == current_collector_type ), ] %>% .[ , c( 'V1', 'V4', 'V5', 'V6' ) ] %>% `colnames<-` ( NULL ) %>%
Filter( function(x){ !all(is.na(x)) }, . ) %>% split( 1:nrow( . ) )
return( collector_df )
})
names( df_for_type_all ) = unique_collectors
return( df_for_type_all )
}
CSV1 = All_Type_df( csv_path )
str( CSV1 )
head( CSV1 )
raw_data = CSV1
raw_data$gy$`1`
subassembly_instance_list_info
str( subassembly_instance_list_info )
machine_info$models$assembly$subAssemblyInstances
str( machine_info$models$assembly$subAssemblyInstances )
current_subassembly_instance = machine_info$models$assembly$subAssemblyInstances[[1]]
str( current_subassembly_instance )
current_subassembly_instance_name = current_subassembly_instance$name ; current_subassembly_name = current_subassembly_instance$subAssembly$name
current_collectors_list = current_subassembly_instance$subAssembly$collectors
current_subassembly_instance_name
current_subassembly_name
current_collectors_list
current_collectors_name = sapply( current_collectors_list, function( current_collector ){
current_collector_name = current_collector[["name"]]
return( current_collector_name )
}, USE.NAMES = F )
str( current_collectors_name )
subassembly_instance_list_info = lapply( machine_info$models$assembly$subAssemblyInstances, function( current_subassembly_instance ){
current_subassembly_instance_name = current_subassembly_instance$name ; current_subassembly_name = current_subassembly_instance$subAssembly$name
current_collectors_list = current_subassembly_instance$subAssembly$collectors
current_collectors_name = sapply( current_collectors_list, function( current_collector ){
current_collector_name = current_collector[["name"]]
return( current_collector_name )
}, USE.NAMES = F )
return( list( 'subassembly_instance' = current_subassembly_instance_name, 'subassembly' = current_subassembly_name, 'collectors' = current_collectors_name ) )
} )
str( subassembly_instance_list_info )
install.packages( 'rmarkdown')
install.packages("rmarkdown")
?fix
?dim
library( rmarkdown)
install.packages("tidyverse")
rm( list = ls() )
food = read.csv( 'D:/projects/James Collin/Food_Inspections.csv', stringsAsFactors = F )
library( dplyr )
head( food )
View( head( food ))
food_0 = read.csv( 'D:/projects/James Collin/Food_Inspections.csv', stringsAsFactors = F )
names( food_0 )
food = food_0 %>%
select( c( 'DBA.Name', 'Facility.Type', 'Risk', 'Zip', 'Inspection.Type', 'Results', 'Violations' ) ) %>%
filter( Facility.Type == 'Restaurant' )
View( food )
?sweep
food$Results = ifelse( food$Results %in% c( 'Out of Business', 'Not Ready' ), NA, food$Results )
food = food_0 %>%
select( c( 'DBA.Name', 'Facility.Type', 'Risk', 'Zip', 'Inspection.Type', 'Results', 'Violations' ) ) %>%
filter( Facility.Type == 'Restaurant' )
?gsub
?read.csv
View( food )
food_0 = read.csv( 'D:/projects/James Collin/Food_Inspections.csv', na.strings = c( 'Out of Business', 'Not Ready' )
stringsAsFactors = F )
food_0 = read.csv( 'D:/projects/James Collin/Food_Inspections.csv', na.strings = c( 'Out of Business', 'Not Ready' ),
stringsAsFactors = F )
food = food_0 %>%
select( c( 'DBA.Name', 'Facility.Type', 'Risk', 'Zip', 'Inspection.Type', 'Results', 'Violations' ) ) %>%
filter( Facility.Type == 'Restaurant' )
View( food )
?summarise
num_of_restaurants_zip_code = food %>%
group_by( Zip ) %>%
summarise( num_FREQ = n() )
num_of_restaurants_zip_code
num_of_restaurants_zip_code = food %>%
group_by( Zip ) %>%
summarise( num_restaurants = n() )
View( num_of_restaurants_zip_code )
unique( food$Results )
num_of_restaurants_passed = food %>% filter( Results == 'Pass' ) %>% nrow()
num_of_restaurants_passed
unique( food$Results )
( num_of_restaurants_passed = food %>% filter( Results == 'Pass' ) %>% nrow() )
( num_of_restaurants_passed = food %>% filter( Results == 'Pass' ) %>% nrow() )
( num_of_restaurants_passed_with_conditions = food %>% filter( Results == 'Pass w/ Conditions' ) %>% nrow() )
( num_of_restaurants_failed = food %>% filter( Results == 'Fail' ) %>% nrow() )
unique( food$Risk )
( num_of_restaurants_risk_low = food %>% filter( Risk == 'Risk 3 (Low)' ) %>% nrow() )
( num_of_restaurants_risk_medium = food %>% filter( Risk == 'Risk 2 (Medium)' ) %>% nrow() )
( num_of_restaurants_risk_high = food %>% filter( Risk == 'Risk 1 (High)' ) %>% nrow() )
risk_score_1_data = food %>% filter( Risk == 'Risk 1 (High)' )
View( risk_score_1_data )
risk_score_1_data = food %>% filter( Risk == 'Risk 1 (High)' & Results %in% c( 'Pass', 'Fail' ) )
View( risk_score_1_data )
library( dplyr ) ; library( ggplot2 )
risk_score_1_data = food %>% filter( Risk == 'Risk 1 (High)' & Results %in% c( 'Pass', 'Fail' ) ) %>%
group_by( Results )
risk_score_1_data = food %>% filter( Risk == 'Risk 1 (High)' & Results %in% c( 'Pass', 'Fail' ) ) %>%
group_by( Results ) %>% summarise( num_res = n() )
risk_score_1_data
risk_score_1_data$Results = as.factor( risk_score_1_data$Results )
ggplot(data=risk_score_1_data, aes(x=Results, y=num_res, fill=Results)) +
geom_bar(colour="black", stat="identity") +
guides(fill=FALSE)
risk_score_pass_fail_data = food %>%
filter( ( Risk %in% c( 'Risk 3 (Low)', 'Risk 2 (Medium)', 'Risk 1 (High)' ) ) & ( Results %in% c( 'Pass', 'Fail', 'Pass w/ Conditions' ) ) )
View( risk_score_pass_fail_data )
install.packages("vcd")
library( vcd )
data("Arthritis")
View( data("Arthritis") )
View( Arthritis )
str( Arthritis )
tab <- xtabs(~Improved + Treatment, data = Arthritis)
tab
summary(assocstats(tab))
assocstats(UCBAdmissions)
View( risk_score_pass_fail_data )
tab = xtabs( ~ Risk + Results, data = risk_score_pass_fail_data )
tab = xtabs( ~ Risk + Results, data = risk_score_pass_fail_data )
tab
summary(assocstats(tab))
( tab = xtabs( ~ Risk + Results, data = risk_score_pass_fail_data ) )
summary(assocstats(tab))
catcorrm <- function(vars, dat) sapply(vars, function(y) sapply(vars, function(x) assocstats(table(dat[,x], dat[,y]))$cramer))
catcorrm( c( 'Risk', 'Results' ), tab )
str( tab )
mm = as.data.frame( tab )
mm
tab
plot( tab )
qplot( tab )
unique( food$Zip )
?arrange
risk_zip_data_0 = food %>% filter( Results %in% c( 'Pass', 'Fail', 'Pass w/ Conditions' ) ) %>% arrange( Zip )
View( risk_zip_data_0 )
risk_zip_data_0$Zip
unique( risk_zip_data_0$Zip )
?mutate_if
risk_zip_data = risk_zip_data_0 %>%
mutate( Zip_category = ifelse( Zip < 60607, 'North', ifelse( Zip < 60614, 'South', ifelse( Zip < 60621, 'East', 'West' ) ) ) )
View( risk_zip_data )
risk_zip_data_1 = risk_zip_data_0 %>%
mutate( Zip_category = ifelse( Zip < 60607, 'North', ifelse( Zip < 60614, 'South', ifelse( Zip < 60621, 'East', 'West' ) ) ) )
results_zip_data = results_zip_data_0 %>%
mutate( Zip_category = ifelse( Zip < 60607, 'North', ifelse( Zip < 60614, 'South', ifelse( Zip < 60621, 'East', 'West' ) ) ) )
results_zip_data_0 = food %>% filter( Results %in% c( 'Pass', 'Fail', 'Pass w/ Conditions' ) ) %>% arrange( Zip )
results_zip_data = results_zip_data_0 %>%
mutate( Zip_category = ifelse( Zip < 60607, 'North', ifelse( Zip < 60614, 'South', ifelse( Zip < 60621, 'East', 'West' ) ) ) )
View( results_zip_data)
results_zip_data_0 = food %>% filter( Results %in% c( 'Pass', 'Fail', 'Pass w/ Conditions' ) ) %>% arrange( Zip )
results_zip_data = results_zip_data_0 %>%
mutate( Zip_category = ifelse( Zip < 60607, 'North', ifelse( Zip < 60614, 'South', ifelse( Zip < 60621, 'East', 'West' ) ) ) )
( results_zip_categorical_data = xtabs( ~ Zip_category + Results, data = results_zip_data ) )
summary(assocstats(results_zip_categorical_data))
plot( results_zip_categorical_data )
( results_zip_categorical_data = xtabs( ~ Zip_category + Results, data = results_zip_data ) )
summary(assocstats(results_zip_categorical_data))
Sys.Date()
knitr::opts_chunk$set(echo = TRUE)
install.packages( "tidyverse" )
rm( list =ls() )
library( forecast ); library( data.table ); library( mgcv ); library( reshape ) ; library( ggplot2 ) ; library( ggthemes )
library( plotly )
df = read.csv( "D:/projects/Karteek/1045401.csv" )
#g = gamm( y ~ s( Date ), data = df )
start_time = c( 2013, 4, 1 ); end_time = c( 2018, 1, 30 )
#...stl fitting
myts = ts( df$y, frequency = 365, start = start_time, end = end_time ) #.....deltat = 1/365 ==> fraction of the sampling period between successive observations
# myts = df$y[  - which( is.na(df$y) == TRUE) ]
stl = stl( myts, t.window=365, s.window="per", robust=TRUE); plot( stl )
tdf = data.frame( stl$time.series ); trend = tdf$trend
arima = auto.arima( myts[1:1401] ); arima_forecast = forecast( myts [1402:1766] ); point_forecast = arima_forecast$model$fitted
arima_predict_df = data.frame( forecast = point_forecast, original = myts [1402:1766] ); residual = arima_forecast$model$residuals; sd = sd(residual)
forecast_fit = ets( myts[1:1401], model = "ZZZ", damped = NULL )
forecast_predict = forecast( myts [1402:1766], model = forecast_fit ); predict = forecast_predict$model$fitted
diff_obs_predict = forecast_predict$model$residuals
original_predict_df = data.frame( observations = seq_along(df$y[1402:1766]) , original_value = df$y[1402:1766], forcasted_value = as.numeric( point_forecast ), predicted_value = as.numeric(predict) )
original_predict_df$forecast = ( original_predict_df$forcasted_value + original_predict_df$predicted_value )/2
t = c( original_predict_df$original_value, original_predict_df$forcasted_value )
max_t = max( t )
min_t = min( t )
p = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forcasted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_t-10, max_t+10 ) + ylab(label="Original & Forcasted Value( ARIMA )") + xlab("Observations") + theme_economist()
ggplotly(p)
s = c( original_predict_df$original_value, original_predict_df$predicted_value )
max_s = max( s )
min_s = min( s )
q = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = predicted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_s-10, max_s+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly(q)
u = c( original_predict_df$original_value, original_predict_df$forecast )
max_u = max( u )
min_u = min( u )
v = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forecast), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_u-10, max_u+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly( v )
#plot(mystl, col = "blue")
#df = data.frame( mystl$time.series )
#plot(df$trend, xlab = "Observations with time", ylab = "Trend", main = "Time serie smoothened trend",type = "l", col = "blue")
View( original_predict_df )
tt = original_predict_df$forcasted_value
tt = abs( original_predict_df$forcasted_value - original_predict_df$original_value )
tt[ tt > 10 ]
tt_2 = abs( original_predict_df$predicted_value - original_predict_df$original_value )
tt_2[ tt_2 > 10 ]
tt_3 = abs( original_predict_df$forecast - original_predict_df$original_value )
tt_3[ tt_3 > 10 ]
length( tt_3[ tt_3 > 10 ] )
rm( list =ls() )
library( forecast ); library( data.table ); library( mgcv ); library( reshape ) ; library( ggplot2 ) ; library( ggthemes )
library( plotly )
df = read.csv( "D:/projects/Karteek/Book1.csv" )
#g = gamm( y ~ s( Date ), data = df )
start_time = c( 2013, 4, 1 ); end_time = c( 2018, 1, 30 )
#...stl fitting
myts = ts( df$y, frequency = 365, start = start_time, end = end_time ) #.....deltat = 1/365 ==> fraction of the sampling period between successive observations
# myts = df$y[  - which( is.na(df$y) == TRUE) ]
stl = stl( myts, t.window=365, s.window="per", robust=TRUE); plot( stl )
tdf = data.frame( stl$time.series ); trend = tdf$trend
arima = auto.arima( myts[1:1401] ); arima_forecast = forecast( myts [1402:1766] ); point_forecast = arima_forecast$model$fitted
arima_predict_df = data.frame( forecast = point_forecast, original = myts [1402:1766] ); residual = arima_forecast$model$residuals; sd = sd(residual)
forecast_fit = ets( myts[1:1401], model = "ZZZ", damped = NULL )
forecast_predict = forecast( myts [1402:1766], model = forecast_fit ); predict = forecast_predict$model$fitted
diff_obs_predict = forecast_predict$model$residuals
original_predict_df = data.frame( observations = seq_along(df$y[1402:1766]) , original_value = df$y[1402:1766], forcasted_value = as.numeric( point_forecast ), predicted_value = as.numeric(predict) )
original_predict_df$forecast = ( original_predict_df$forcasted_value + original_predict_df$predicted_value )/2
t = c( original_predict_df$original_value, original_predict_df$forcasted_value )
max_t = max( t )
min_t = min( t )
p = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forcasted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_t-10, max_t+10 ) + ylab(label="Original & Forcasted Value( ARIMA )") + xlab("Observations") + theme_economist()
ggplotly(p)
s = c( original_predict_df$original_value, original_predict_df$predicted_value )
max_s = max( s )
min_s = min( s )
q = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = predicted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_s-10, max_s+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly(q)
u = c( original_predict_df$original_value, original_predict_df$forecast )
max_u = max( u )
min_u = min( u )
v = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forecast), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_u-10, max_u+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly( v )
#plot(mystl, col = "blue")
#df = data.frame( mystl$time.series )
#plot(df$trend, xlab = "Observations with time", ylab = "Trend", main = "Time serie smoothened trend",type = "l", col = "blue")
tt_3 = abs( original_predict_df$forecast - original_predict_df$original_value )
tt_3[ tt_3 > 10 ]
tt_2 = abs( original_predict_df$predicted_value - original_predict_df$original_value )
tt_2[ tt_2 > 10 ]
tt = abs( original_predict_df$forcasted_value - original_predict_df$original_value )
tt[ tt > 10 ]
rm( list =ls() )
library( forecast ); library( data.table ); library( mgcv ); library( reshape ) ; library( ggplot2 ) ; library( ggthemes )
library( plotly )
df = read.csv( "D:/projects/Karteek/Book2.csv" )
#g = gamm( y ~ s( Date ), data = df )
start_time = c( 2013, 4, 1 ); end_time = c( 2018, 1, 30 )
#...stl fitting
myts = ts( df$y, frequency = 365, start = start_time, end = end_time ) #.....deltat = 1/365 ==> fraction of the sampling period between successive observations
# myts = df$y[  - which( is.na(df$y) == TRUE) ]
stl = stl( myts, t.window=365, s.window="per", robust=TRUE); plot( stl )
tdf = data.frame( stl$time.series ); trend = tdf$trend
arima = auto.arima( myts[1:1401] ); arima_forecast = forecast( myts [1402:1766] ); point_forecast = arima_forecast$model$fitted
arima_predict_df = data.frame( forecast = point_forecast, original = myts [1402:1766] ); residual = arima_forecast$model$residuals; sd = sd(residual)
forecast_fit = ets( myts[1:1401], model = "ZZZ", damped = NULL )
forecast_predict = forecast( myts [1402:1766], model = forecast_fit ); predict = forecast_predict$model$fitted
diff_obs_predict = forecast_predict$model$residuals
original_predict_df = data.frame( observations = seq_along(df$y[1402:1766]) , original_value = df$y[1402:1766], forcasted_value = as.numeric( point_forecast ), predicted_value = as.numeric(predict) )
original_predict_df$forecast = ( original_predict_df$forcasted_value + original_predict_df$predicted_value )/2
t = c( original_predict_df$original_value, original_predict_df$forcasted_value )
max_t = max( t )
min_t = min( t )
p = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forcasted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_t-10, max_t+10 ) + ylab(label="Original & Forcasted Value( ARIMA )") + xlab("Observations") + theme_economist()
ggplotly(p)
s = c( original_predict_df$original_value, original_predict_df$predicted_value )
max_s = max( s )
min_s = min( s )
q = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = predicted_value), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_s-10, max_s+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly(q)
u = c( original_predict_df$original_value, original_predict_df$forecast )
max_u = max( u )
min_u = min( u )
v = ggplot( original_predict_df, aes(x = observations ) ) + geom_line(aes(y = forecast), colour="blue", size = 1 ) +
geom_line(aes(y = original_value), colour = "grey", size = 1 ) + ylim( min_u-10, max_u+10 ) + ylab(label="Original & Forcasted Value( ets )") + xlab("Observations") + theme_economist()
ggplotly( v )
#plot(mystl, col = "blue")
#df = data.frame( mystl$time.series )
#plot(df$trend, xlab = "Observations with time", ylab = "Trend", main = "Time serie smoothened trend",type = "l", col = "blue")
tt = abs( original_predict_df$forcasted_value - original_predict_df$original_value )
tt[ tt > 10 ]
tt_2 = abs( original_predict_df$predicted_value - original_predict_df$original_value )
tt_2[ tt_2 > 10 ]
tt_3 = abs( original_predict_df$forecast - original_predict_df$original_value )
tt_3[ tt_3 > 10 ]
View( df )
plot( df$y, df$festive )
rm( list =ls() )
## import library
library(dplyr); library(DT); library(quantmod); library(plotly); library(forecast); library(tseries); library(TSA)
library(gtools); library(reshape)
Product = read.csv("D:/projects/Karteek/Product_0.csv") ## import the data set
range1=365  ### Give input range for forecasted data
##### Functions for forecasting #####
model_forecasting_data=function( product_data, range, range1 ){
dt = product_data[ complete.cases( product_data ), ]
dt_train=dt[ 1:( nrow( dt ) - range ), ]     ## train data set on which we build the model
dt_train$seasonal_adj = mean( dt_train[ , 2 ] )    ## ADJUSTING SEASONAL MEANS
ForeCalc = function( data, model, trns, season ){
data$Fitted = as.numeric( model$fitted )
data$Residuals = as.numeric( residuals( model ) )
data$Abs_Pct_Err = as.numeric( abs( data$Residuals / data[ , 2 ] )*100 )
Err_Var = var( data$Residuals )
ftd = model$fitted
Fitted = forecast( ftd, range1 )
plot( Fitted )
Fitted = as.numeric(Fitted$mean)
Fitted = data.frame(Fitted)
data <- smartbind(data,Fitted) # Attached forecasted value with Fitted column
######### ForeCalc FOR LOG DATA ##########
if ( trns==1 ){ data$Fitted = exp(data$Fitted+Err_Var/2) } # For Log Transformation
else if( trns==2 ){ data$Fitted = (data$Fitted^2 + Err_Var) } # For Sqrt Transformation
if( season == 1 ){ data$Fitted = (data$Fitted + data$V1) - mean(data$V1) }  # For Additive Seasonality
if( season == 2 ){ data$Fitted = (data$Fitted * data$V1) / mean(data$V1) }  # For Multiplicative Seasonality
data$Residuals = as.numeric( data[,2] )- data$Fitted
data$Abs_Pct_Err = abs( data$Residuals/as.numeric( data[,2] ) )*100
ErrorSum = sum(data$Abs_Pct_Err,na.rm=TRUE)
MAPE = ErrorSum/nrow(data)
data = data.frame(data,MAPE)
return(data)
}
arima_input = ts( dt_train[ , 2 ], frequency = 365, start = c( 2013, 4, 1 ) )
model1 = auto.arima( log( arima_input ), approximation = FALSE, trace = TRUE, ic = "aic" )
finaldata1 = ForeCalc( dt_train, model1, 1, 0 )
model4 = auto.arima( arima_input, approximation = FALSE, trace = TRUE,ic = "aic" )
finaldata4 = ForeCalc( dt_train, model4, 0, 0 )
model7 = auto.arima( sqrt( arima_input ), approximation = FALSE, trace = TRUE,ic = "aic" )
finaldata7 = ForeCalc( dt_train, model7, 2, 0 )
model10 = auto.arima( arima_input, lambda = BoxCox.lambda( arima_input ) )    ## AUTO ARIMA USING BOXCOX ##
finaldata10 = ForeCalc( dt_train, model10 , 0, 0 )
# head(finaldata10)
latestdata = rbind( finaldata1, finaldata4, finaldata7, finaldata10 )
assign( "latestdata ", latestdata , envir = .GlobalEnv )
finaldata <- latestdata[ which( latestdata$MAPE == min( latestdata$MAPE ) ), ]
return(finaldata)
}
range = 0
#### call that model_forecasting_function
result=model_forecasting_data( Product, range, range1 )
result1=data.frame( result )
actual_fitted_data = result1[ 1:( nrow( Product ) - range ) , c( "Date", "y", "Fitted" ) ]
future_forecasting_df = data.frame( Future_fitted_values = result1[ ( nrow( Product ) - range + 1 ):nrow( result1 ), c( "Fitted" ) ] )
str( result )
rm( list = ls() )
install.packages( "devtools" )   #... installing devtools
devtools::install_github('talgalili/heatmaply')
library( heatmaply )   #... calling heatmaply
df = read.csv( 'D:/projects/James Collin/heatmap/sample_feb9.csv' )    #... reading data
#.... renaming column names ......
names( df ) = c( 'city', 'state', 'num_of_store', 'num_of_total_customer', 'num_of_our_customers', 'percent' )
row_names = paste0( df$city, '-', df$state )   #.... creating row names
#..... selecting values to show on heatmap
df_1 = df %>% select( 'num_of_store', 'num_of_total_customer', 'num_of_our_customers', 'percent' )
rownames( df_1 ) = row_names   #... renaming rows
suppressMessages( heatmaply( df_1 ) )   #.... creating heatmap
?heatmaply
##store some useful variables
t0 <- 300
g <- 9.8
h1 <- 0.5
h2 <- 1.25
rho <- 1.3
cp <- 1005
cd <- 0.6
af <- 2
ai <- 4
i <- 100
## First define get Delta T fr case 1 and case 2 holding all but h constant
DeltaT <- function(h1, h2) {
DT1 <- ((t0/(2*g*h1))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
DT2 <- ((t0/(2*g*h2))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
print(DT1)
print(DT2)
}
## now make a function to vary h
h <- seq(1, 10, 0.1)
VarH <- function(h){
DT <- ((t0/(2*g*h))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
}
DTH <- lapply(h, VarH)
DTH
install.packages("d3heatmap")
library( d3heatmap )
d3heatmap(nba_players, scale = "column")
d3heatmap(df_1, scale = "column")
str( df_1 )
d3heatmap(df_1)
d3heatmap(df_1[,-4])
##store some useful variables
t0 <- 300
g <- 9.8
h1 <- 0.5
h2 <- 1.25
rho <- 1.3
cp <- 1005
cd <- 0.6
af <- 2
ai <- 4
i <- 100
## First define get Delta T fr case 1 and case 2 holding all but h constant
DeltaT <- function(h1, h2) {
DT1 <- ((t0/(2*g*h1))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
DT2 <- ((t0/(2*g*h2))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
print(DT1)
print(DT2)
}
## now make a function to vary h
h <- seq(1, 10, 0.1)
VarH <- function(h){
DT <- ((t0/(2*g*h))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
}
DTH <- lapply(h, VarH)
## now co-vary air flow area (afvar) and T0 (t0var)
t0var <- seq(260, 320)
afvar <- seq(0.001, 2, length.out = length(t0var))
Varft <- function(afvar, t0var){
DT <- ((t0var/(2*g*h))^(1/3))*((i*ai/(rho*cp*cd*afvar))^(2/3))
}
z    <- outer(afvar, t0var, FUN="zfun")
persp(x, y, z)
z    <- outer(afvar, t0var, FUN="Varft")
persp(x, y, z)
persp(t0var, afvar, z)
##store some useful variables
t0 <- 300
g <- 9.8
h1 <- 0.5
h2 <- 1.25
rho <- 1.3
cp <- 1005
cd <- 0.6
af <- 2
ai <- 4
i <- 100
## First define get Delta T fr case 1 and case 2 holding all but h constant
DeltaT <- function(h1, h2) {
DT1 <- ((t0/(2*g*h1))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
DT2 <- ((t0/(2*g*h2))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
print(DT1)
print(DT2)
}
## now make a function to vary h
h <- seq(1, 10, 0.1)
VarH <- function(h){
DT <- ((t0/(2*g*h))^(1/3))*((i*ai/(rho*cp*cd*af))^(2/3))
}
DTH <- lapply(h, VarH)
## now co-vary air flow area (afvar) and T0 (t0var)
t0var <- seq(260, 320)
afvar <- seq(0.001, 2, length.out = length(t0var))
Varft <- function(afvar, t0var){
DT <- ((t0var/(2*g*h))^(1/3))*((i*ai/(rho*cp*cd*afvar))^(2/3))
}
z    <- outer(afvar, t0var, FUN="Varft")
persp(t0var, afvar, z)
library( heatmaply )   #... calling heatmaply
df_1$percent[1]
?gsub
tt = gsub( '%', '', df_1$percent )
tt
as.integer(10.2)
as.integer(10.7)
d3heatmap(df_1, scale = "column")
df_1$percent = as.numeric( gsub( '%', '', df_1$percent ) )
library( d3heatmap )
d3heatmap(df_1, scale = "column")
setwd("D:/projects/James Collin/shiny dashboard/18p1data")
mm = read.csv( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', sep = '/t' )
?read.csv
mm = read.csv( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', sep = "/t" )
mm = read.csv( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', sep = "\t" )
mm = read.table( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', sep = "\t" )
mm = read.table( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', sep = "\t", header = T )
mm = read.table( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', skip = 1 )
View( mm )
mm = read.table( 'D:/projects/James Collin/shiny dashboard/18p1data/choleraDeaths.tsv', header = T )
View( mm )
